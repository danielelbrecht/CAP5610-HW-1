{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAP5610_HW1Problem3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielelbrecht/CAP5610-HW-1/blob/master/CAP5610_HW1Problem3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7DEeYaWiqczS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Single layer digit classifier using softmax activation and categorical cross entropy loss"
      ]
    },
    {
      "metadata": {
        "id": "sGh_heRqqmMe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Import data\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from keras.datasets import mnist\n",
        "\n",
        "#Load data\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMVU754rqyJs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Preprocess data\n",
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "\n",
        "\n",
        "#Use one-hot encoding\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels_original)\n",
        "test_labels = to_categorical(test_labels_original)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Jmf5t8xrIJ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Returns the ativations for an entire mini batch\n",
        "def softmax(weights, data):\n",
        "  \n",
        "  activations = []\n",
        "  \n",
        "  for i in range(len(data)):\n",
        "    total = 0\n",
        "    temp = []\n",
        "    for j in range(10):\n",
        "      total = total + np.exp(np.dot(weights[j], data[i]))\n",
        "      temp.append(np.exp(np.dot(weights[j], data[i])))\n",
        "    temp = temp / total\n",
        "    activations.append(temp)\n",
        "    \n",
        "  return activations\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "#gradient function for softmax activation with log loss = y(a-k)*X\n",
        "def gradient(weights, bias, x, y):\n",
        "  \n",
        "  #Get softmax activations for mini batch\n",
        "  activations = softmax(weights, x)\n",
        "  \n",
        "  #Calculate dL/dz\n",
        "  #Calculate each partial derivative\n",
        "  for i in range(10):\n",
        "    \n",
        "    \n",
        "    #Subtract Kronecker Delta\n",
        "    for j in range(len(activations)):\n",
        "      activations[j][i] = activations[j][i]-1\n",
        "\n",
        "  z = np.multiply(y, activations)\n",
        " \n",
        "  \n",
        "  return np.dot(np.transpose(z), x)\n",
        "\n",
        "def bias_gradient():\n",
        "  return\n",
        "\n",
        "#Calculat cce loss\n",
        "def cce_loss(weights, bias, x, y):\n",
        "  \n",
        "  length = len(y)\n",
        "  loss = 0\n",
        "  predictions = softmax(weights, data)\n",
        "  \n",
        "  for i in range(length):\n",
        "    temp = 0\n",
        "    for j in range(10):\n",
        "      temp = temp + y[i][j] * np.log(predictions[i][j])\n",
        "      \n",
        "    loss = loss - temp\n",
        "      \n",
        "  return loss / float(length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qa6vm_rWq1Cp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "\n",
        "#Initialize weight matrix\n",
        "#w[0][50] represents the 50th weight of the 0th neuron\n",
        "weights = []\n",
        "bias = []\n",
        "\n",
        "for i in range(10):\n",
        "  weights.append([])\n",
        "  bias.append(random.random()-0.5)\n",
        "  for j in range(784):\n",
        "    weights[i].append(random.random()-0.5)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n_YYQjUckyqN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1cfd8f9a-1757-47c0-9a6f-570eab9010a9"
      },
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "activations = softmax(weights, train_images[2:10])\n",
        "print(activations[0])\n",
        "print(train_labels[0])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00129354 0.00322293 0.03930194 0.33505381 0.0004036  0.03187763\n",
            " 0.29542617 0.28246344 0.0020157  0.00894123]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D94Z5NgK9g-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        },
        "outputId": "c2c2fcb2-8339-4f81-f04d-8ce739680f19"
      },
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "#Define hyperparameters\n",
        "epochs = 1\n",
        "batch_size = 1000\n",
        "learning_rate = 0.01\n",
        "length = len(train_images)\n",
        "\n",
        "\n",
        "#Iterate over epochs\n",
        "for i in range(epochs):\n",
        "  \n",
        "  #Iterate over mini batches\n",
        "  for j in range(int(length/batch_size)):\n",
        "    \n",
        "    \n",
        "    #get mini batch\n",
        "    data = train_images[batch_size*j:min(length, batch_size*(j+1))]\n",
        "    labels = train_labels[batch_size*j:min(length, batch_size*(j+1))]\n",
        "    \n",
        "    #Update weights\n",
        "    weights = weights - learning_rate*gradient(weights, bias, data, labels)\n",
        "    #bias = bias - learning_rate * bias_gradient(weight, bias, data, labels)\n",
        "    \n",
        "    print(\"Mini batch %i complete\" % j)\n",
        "    \n",
        "    #measure loss after every 10,000 data points\n",
        "    if batch_size * j % 10000 == 0:\n",
        "      print(\"Loss = %i\" % cce_loss(weights, bias, data, labels))\n",
        "      \n",
        "  print('Epoch %i complete' % i)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mini batch 0 complete\n",
            "Loss = 32\n",
            "Mini batch 1 complete\n",
            "Mini batch 2 complete\n",
            "Mini batch 3 complete\n",
            "Mini batch 4 complete\n",
            "Mini batch 5 complete\n",
            "Mini batch 6 complete\n",
            "Mini batch 7 complete\n",
            "Mini batch 8 complete\n",
            "Mini batch 9 complete\n",
            "Mini batch 10 complete\n",
            "Loss = 35\n",
            "Mini batch 11 complete\n",
            "Mini batch 12 complete\n",
            "Mini batch 13 complete\n",
            "Mini batch 14 complete\n",
            "Mini batch 15 complete\n",
            "Mini batch 16 complete\n",
            "Mini batch 17 complete\n",
            "Mini batch 18 complete\n",
            "Mini batch 19 complete\n",
            "Mini batch 20 complete\n",
            "Loss = 32\n",
            "Mini batch 21 complete\n",
            "Mini batch 22 complete\n",
            "Mini batch 23 complete\n",
            "Mini batch 24 complete\n",
            "Mini batch 25 complete\n",
            "Mini batch 26 complete\n",
            "Mini batch 27 complete\n",
            "Mini batch 28 complete\n",
            "Mini batch 29 complete\n",
            "Mini batch 30 complete\n",
            "Loss = 31\n",
            "Mini batch 31 complete\n",
            "Mini batch 32 complete\n",
            "Mini batch 33 complete\n",
            "Mini batch 34 complete\n",
            "Mini batch 35 complete\n",
            "Mini batch 36 complete\n",
            "Mini batch 37 complete\n",
            "Mini batch 38 complete\n",
            "Mini batch 39 complete\n",
            "Mini batch 40 complete\n",
            "Loss = 32\n",
            "Mini batch 41 complete\n",
            "Mini batch 42 complete\n",
            "Mini batch 43 complete\n",
            "Mini batch 44 complete\n",
            "Mini batch 45 complete\n",
            "Mini batch 46 complete\n",
            "Mini batch 47 complete\n",
            "Mini batch 48 complete\n",
            "Mini batch 49 complete\n",
            "Mini batch 50 complete\n",
            "Loss = 32\n",
            "Mini batch 51 complete\n",
            "Mini batch 52 complete\n",
            "Mini batch 53 complete\n",
            "Mini batch 54 complete\n",
            "Mini batch 55 complete\n",
            "Mini batch 56 complete\n",
            "Mini batch 57 complete\n",
            "Mini batch 58 complete\n",
            "Mini batch 59 complete\n",
            "Epoch 0 complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nXR7C3xf6S88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "e1be7b04-5e2e-450b-c893-f5b8127d4e7d"
      },
      "cell_type": "code",
      "source": [
        "predictions = softmax(weights, train_images[1:20])\n",
        "\n",
        "for i in range(10):\n",
        "  print(predictions[i])\n",
        "  print(np.argmax(train_labels[i]))\n",
        "  print()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.89632552e-26 8.35288350e-18 5.15255647e-18 6.00680312e-15\n",
            " 4.62162937e-15 1.39648571e-18 1.18043318e-16 1.20266203e-17\n",
            " 8.09138522e-17 1.00000000e+00]\n",
            "5\n",
            "\n",
            "[2.62461705e-13 1.95533536e-09 2.64423352e-09 2.72384413e-07\n",
            " 3.22824995e-10 2.91784641e-09 2.50713019e-08 4.25338651e-08\n",
            " 1.80782673e-09 9.99999650e-01]\n",
            "0\n",
            "\n",
            "[6.50474387e-13 1.06222818e-10 1.08532904e-09 4.21901324e-10\n",
            " 1.28829450e-09 3.62622192e-11 5.59691151e-10 8.14472463e-11\n",
            " 1.79914176e-11 9.99999996e-01]\n",
            "4\n",
            "\n",
            "[8.28424889e-23 4.43528683e-20 1.50601232e-20 3.16791319e-18\n",
            " 5.92332580e-19 1.05611707e-20 6.46392125e-20 1.10063815e-19\n",
            " 7.61639852e-21 1.00000000e+00]\n",
            "1\n",
            "\n",
            "[1.22544878e-24 2.14903478e-18 1.80528444e-20 1.12694069e-18\n",
            " 5.22586095e-18 5.34707249e-20 1.64641332e-18 9.30623924e-18\n",
            " 3.35612097e-20 1.00000000e+00]\n",
            "9\n",
            "\n",
            "[3.34843111e-15 8.08014921e-13 2.15487789e-11 9.72541860e-13\n",
            " 1.16202732e-12 5.42639517e-12 2.34003482e-13 2.02897734e-12\n",
            " 2.96239560e-12 1.00000000e+00]\n",
            "2\n",
            "\n",
            "[1.03264501e-29 2.76996300e-21 3.46996253e-22 9.88910481e-22\n",
            " 4.00311342e-22 1.03323491e-23 2.64128271e-22 1.44963071e-20\n",
            " 2.52981142e-25 1.00000000e+00]\n",
            "1\n",
            "\n",
            "[3.41383821e-10 5.96398909e-09 4.53114114e-07 1.31855323e-08\n",
            " 1.10642743e-08 1.19584441e-07 3.88400005e-09 7.18827696e-09\n",
            " 3.99811146e-08 9.99999346e-01]\n",
            "3\n",
            "\n",
            "[6.11691083e-20 1.52963124e-16 1.44280959e-15 5.04595876e-15\n",
            " 1.24421746e-16 1.75227956e-15 4.87698305e-16 1.59134795e-15\n",
            " 1.26675384e-16 1.00000000e+00]\n",
            "1\n",
            "\n",
            "[2.25794844e-24 1.06349957e-17 2.09858834e-19 1.74442410e-18\n",
            " 3.32864369e-18 2.20102321e-18 5.80136907e-19 8.79572509e-20\n",
            " 1.15912286e-20 1.00000000e+00]\n",
            "4\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}