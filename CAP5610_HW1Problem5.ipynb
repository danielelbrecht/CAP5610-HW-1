{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAP5610_HW1Problem5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielelbrecht/CAP5610-HW-1/blob/master/CAP5610_HW1Problem5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "u_uQh4L3753M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e18dcb10-b5a3-4684-b7bd-6a5c211826cc"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#Load data\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oxlUYK6obzHx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Softmax classifier using Keras.  Additional features added to quantify images"
      ]
    },
    {
      "metadata": {
        "id": "QxlUP_iJ789W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Preprocess data\n",
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0DeeP-38RfY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "classes = 10\n",
        "batch_size = 100\n",
        "\n",
        "#Use one-hot encoding\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels_original)\n",
        "test_labels = to_categorical(test_labels_original)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PSsjCf85b8n3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function to calculate connected components in an image\n",
        "\n",
        "  \n",
        "#Connected components\n",
        "def get_connected_components(image):\n",
        "  \n",
        "  n = 0\n",
        "  labels = np.zeros(shape = (28, 28))\n",
        "  \n",
        "  \n",
        "  #Assign a label to each pixel in image\n",
        "  for i in range(28):\n",
        "    for j in range(28):\n",
        "      \n",
        "      #If pixel is unlabeled, find every connected pixel and mark them as explored\n",
        "      if labels[i][j] == 0 and image[i][j] == 0:\n",
        "        \n",
        "        n = n + 1\n",
        "        x, y = i, j\n",
        "        stack = [[i, j]]\n",
        "        explored = []\n",
        "        \n",
        "        \n",
        "        while stack != []:\n",
        "          \n",
        "          #get neighbors\n",
        "          if [x,max(0,y-1)] not in explored and image[x][max(0,y-1)] == 0:\n",
        "            stack.append([x,max(0,y-1)])\n",
        "          if [x,min(27, y+1)] not in explored and image[x][min(27, y+1)] == 0:\n",
        "            stack.append([x,min(27, y+1)])\n",
        "          if [max(0, x-1),y] not in explored and image[max(0,x-1)][y] == 0:\n",
        "            stack.append([max(0,x-1),y])\n",
        "          if [min(27, x+1),y] not in explored and image[min(27, x+1)][y] == 0:\n",
        "            stack.append([min(27, x+1),y])\n",
        "          \n",
        "          \n",
        "          #Label pixel being popped from stack\n",
        "          labels[x][y] = 1\n",
        "          \n",
        "          #Do updates\n",
        "          temp = stack.pop()\n",
        "          x = temp[0]\n",
        "          y = temp[1]\n",
        "          explored.append(temp)\n",
        "          \n",
        "          \n",
        "        \n",
        "  return n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SXO6AoBHyLu9",
        "colab_type": "code",
        "outputId": "06da4e9f-4f4e-4ba2-e636-44ab1e48cc37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "#Modify data to include new features\n",
        "\n",
        "train_images_modified = []\n",
        "\n",
        "for i in range(10000):\n",
        "  train_images_modified.append(np.append(train_images[i], get_connected_components(train_images_original[i])))\n",
        "  \n",
        "  if i % 1000 == 0:\n",
        "    print(i ,\"examples trained\")\n",
        "                                 \n",
        "                                 \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 examples trained\n",
            "1000 examples trained\n",
            "2000 examples trained\n",
            "3000 examples trained\n",
            "4000 examples trained\n",
            "5000 examples trained\n",
            "6000 examples trained\n",
            "7000 examples trained\n",
            "8000 examples trained\n",
            "9000 examples trained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3AJp2cIc8WBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Convert data to np array\n",
        "train_images_modified_array = np.asarray(train_images_modified)\n",
        "\n",
        "#Define model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(layers.Dense(classes, activation = 'softmax'))\n",
        "\n",
        "model.compile(optimizer = tf.train.AdamOptimizer(learning_rate),\n",
        "                            loss = 'categorical_crossentropy',\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "#Define model to test on regular data for comparison\n",
        "model2 = tf.keras.Sequential()\n",
        "\n",
        "model2.add(layers.Dense(classes, activation = 'softmax'))\n",
        "\n",
        "model2.compile(optimizer = tf.train.AdamOptimizer(learning_rate),\n",
        "                            loss = 'categorical_crossentropy',\n",
        "                            metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwVhFd_V8j6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "eb6c61b4-7a31-44a3-dddd-95d2276a25f7"
      },
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "model.fit(train_images_modified_array, train_labels[0:10000], epochs = epochs, batch_size = batch_size)\n",
        "\n",
        "\n",
        "#Traning of other model for comparison\n",
        "model2.fit(train_images[0:10000], train_labels[0:10000], epochs = epochs, batch_size = batch_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 0s 37us/sample - loss: 0.5100 - acc: 0.8497\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 0s 30us/sample - loss: 0.2896 - acc: 0.9175\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.2483 - acc: 0.9300\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.2280 - acc: 0.9344\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 0.2075 - acc: 0.9388\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 0.2024 - acc: 0.9381\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.1891 - acc: 0.9450\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 0s 28us/sample - loss: 0.1824 - acc: 0.9432\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 0s 26us/sample - loss: 0.1682 - acc: 0.9508\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.1632 - acc: 0.9511\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 0s 34us/sample - loss: 0.5262 - acc: 0.8496\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 0s 26us/sample - loss: 0.3043 - acc: 0.9136\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 0s 26us/sample - loss: 0.2695 - acc: 0.9239\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 0s 25us/sample - loss: 0.2511 - acc: 0.9271\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 0s 25us/sample - loss: 0.2355 - acc: 0.9301\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 0s 25us/sample - loss: 0.2205 - acc: 0.9365\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 0s 25us/sample - loss: 0.2096 - acc: 0.9385\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 0s 25us/sample - loss: 0.2034 - acc: 0.9418\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 0s 26us/sample - loss: 0.1945 - acc: 0.9433\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 0s 25us/sample - loss: 0.1856 - acc: 0.9457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f297a5fdc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}